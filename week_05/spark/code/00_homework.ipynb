{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a98ea3d",
   "metadata": {},
   "source": [
    "## Week 5 Homework\n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "We'll use high volume for-hire vehicles (HVFHV) dataset for that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d60722f",
   "metadata": {},
   "source": [
    "### Question 1. Install Spark and PySpark\n",
    "\n",
    "    Install Spark\n",
    "    Run PySpark\n",
    "    Create a local spark session\n",
    "    Execute spark.version\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83591d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/sinevi/bin/spark-3.0.3-bin-hadoop3.2/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/03/02 15:15:29 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67d6318b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.3'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ac4c02",
   "metadata": {},
   "source": [
    "### Question 2. HVFHW February 2021\n",
    "\n",
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
    "\n",
    "Read it with Spark using the same schema as we did in the lessons.\n",
    "\n",
    "Repartition it to 24 partitions and save it to parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fb0579",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hvfhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('data/raw/hvfhw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1fe2dc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747bb206",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pd = pd.read_csv('data/raw/hvfhw/fhvhv_tripdata_2021-02.csv', nrows = 1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d473f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(hvfhs_license_num,StringType,true),StructField(dispatching_base_num,StringType,true),StructField(pickup_datetime,StringType,true),StructField(dropoff_datetime,StringType,true),StructField(PULocationID,LongType,true),StructField(DOLocationID,LongType,true),StructField(SR_Flag,DoubleType,true)))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.createDataFrame(df_hvfhv_pd).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e88829f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField(\"hvfhs_license_num\", types.StringType(), True),\n",
    "    types.StructField(\"dispatching_base_num\", types.StringType(), True),\n",
    "    types.StructField(\"pickup_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"dropoff_datetime\", types.TimestampType(), True),\n",
    "    types.StructField(\"PULocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"DOLocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"SR_Flag\", types.DoubleType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6ac37",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = f'data/raw/hvfhw/fhvhv_tripdata_2021-02.csv'\n",
    "output_path = f'data/pq/hvfhv/'\n",
    "\n",
    "\n",
    "df_hvfhv = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(input_path)\n",
    "\n",
    "df_hvfhv \\\n",
    "    .repartition(24) \\\n",
    "    .write.parquet(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5141986d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210M\t./data/pq/hvfhv\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./data/pq/hvfhv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb47211",
   "metadata": {},
   "source": [
    "### Question 3. Count records\n",
    "\n",
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "365c1ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq = spark.read.parquet('data/pq/hvfhv/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f043e1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To query df using standard SQL synthax we need to build it on top of the dataframe\n",
    "\n",
    "df_hvfhv_pq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "181f581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a5342c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-02-15\n"
     ]
    }
   ],
   "source": [
    "d = datetime.date(2021, 2, 15)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5decf1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hvfhv_pq.filter(df_hvfhv_pq.pickup_datetime == datetime.date(2021, 2, 15)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c4f02b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f68f8397",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq = df_hvfhv_pq \\\n",
    "    .withColumn(\"pickup_date\", to_date(\"pickup_datetime\")) \\\n",
    "#     .show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce0751eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "367170"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_hvfhv_pq.filter(df_hvfhv_pq.pickup_date == datetime.date(2021, 2, 15)).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1928a83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hvfhv_pq.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7968f790",
   "metadata": {},
   "source": [
    "### Question 4. Longest trip for each day\n",
    "\n",
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efef1765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq = df_hvfhv_pq \\\n",
    "    .withColumn(\"trip_duration\", unix_timestamp(\"dropoff_datetime\") - unix_timestamp(\"pickup_datetime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4782f3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- hvfhs_license_num: string (nullable = true)\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropoff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: double (nullable = true)\n",
      " |-- pickup_date: date (nullable = true)\n",
      " |-- trip_duration: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_hvfhv_pq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02245c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4c4f7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    pickup_datetime, \n",
    "    trip_duration\n",
    "FROM trips_data\n",
    "ORDER BY trip_duration DESC\n",
    "LIMIT 1\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8e0cc866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:>                                                          (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+\n",
      "|    pickup_datetime|trip_duration|\n",
      "+-------------------+-------------+\n",
      "|2021-02-11 13:40:44|        75540|\n",
      "+-------------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:===========================================================(4 + 0) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b6a93b",
   "metadata": {},
   "source": [
    "### Question 5. Most frequent dispatching_base_num\n",
    "\n",
    "Now find the most frequently occurring dispatching_base_num in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "    Note: the answer may depend on how you write the query, so there are multiple correct answers. Select the one you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7864bf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(*) AS cnt_rows,\n",
    "    dispatching_base_num\n",
    "FROM trips_data\n",
    "GROUP BY dispatching_base_num\n",
    "ORDER BY cnt_rows DESC\n",
    "LIMIT 1\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1711c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|cnt_rows|dispatching_base_num|\n",
      "+--------+--------------------+\n",
      "| 3233664|              B02510|\n",
      "+--------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a07e17b",
   "metadata": {},
   "source": [
    "### Question 6. Most common locations pair\n",
    "\n",
    "Find the most common pickup-dropoff pair.\n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4bf9b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "zones_schema = types.StructType([\n",
    "    types.StructField(\"LocationID\", types.IntegerType(), True),\n",
    "    types.StructField(\"Borough\", types.StringType(), True),\n",
    "    types.StructField(\"Zone\", types.StringType(), True),\n",
    "    types.StructField(\"service_zone\", types.StringType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68930beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(zones_schema) \\\n",
    "    .csv('data/raw/taxi+_zone_lookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3029de37",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LocationID: integer (nullable = true)\n",
      " |-- Borough: string (nullable = true)\n",
      " |-- Zone: string (nullable = true)\n",
      " |-- service_zone: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c5c9285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74522187",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq = df_hvfhv_pq \\\n",
    "    .join(df_zone.select(\"LocationID\", \"Zone\") \\\n",
    "          .withColumnRenamed(\"LocationID\",\"PULocationID\") \\\n",
    "          .withColumnRenamed(\"Zone\",\"PUZone\"), \"PULocationID\", \"left_outer\") \\\n",
    "    .join(df_zone.select(\"LocationID\", \"Zone\") \\\n",
    "          .withColumnRenamed(\"LocationID\",\"DOLocationID\") \\\n",
    "          .withColumnRenamed(\"Zone\",\"DOZone\"), \"DOLocationID\", \"left_outer\") \\\n",
    "    .withColumn(\"pickup_dropoff\", concat_ws(' / ', \"PUZone\", \"DOZone\", ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "277313f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 15:31:48 WARN CacheManager: Asked to cache already cached data.\n",
      "22/03/02 15:31:48 WARN MemoryStore: Not enough space to cache rdd_69_0 in memory! (computed 38.8 MiB so far)\n",
      "22/03/02 15:31:48 WARN MemoryStore: Not enough space to cache rdd_69_2 in memory! (computed 38.8 MiB so far)\n",
      "[Stage 20:=================================================>    (185 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|      pickup_dropoff|cnt_rows|\n",
      "+--------------------+--------+\n",
      "|East New York / E...|   45041|\n",
      "|Borough Park / Bo...|   37329|\n",
      "| Canarsie / Canarsie|   28026|\n",
      "|Crown Heights Nor...|   25976|\n",
      "|Bay Ridge / Bay R...|   17934|\n",
      "|Jackson Heights /...|   14688|\n",
      "|   Astoria / Astoria|   14688|\n",
      "|Central Harlem No...|   14481|\n",
      "|Bushwick South / ...|   14424|\n",
      "|Flatbush/Ditmas P...|   13976|\n",
      "+--------------------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_hvfhv_pq \\\n",
    "    .cache() \\\n",
    "    .groupBy(\"pickup_dropoff\") \\\n",
    "    .agg(count(lit(1)).alias(\"cnt_rows\")) \\\n",
    "    .sort(\"cnt_rows\", ascending = False) \\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c32c290",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hvfhv_pq.registerTempTable('trips_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0442544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    count(*) AS cnt_rows,\n",
    "    PUZone,\n",
    "    DOZone,\n",
    "    pickup_dropoff\n",
    "FROM trips_data\n",
    "GROUP BY\n",
    "    PUZone,\n",
    "    DOZone,\n",
    "    pickup_dropoff\n",
    "ORDER BY cnt_rows DESC\n",
    "LIMIT 10\n",
    ";\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "346c8653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/02 15:32:52 WARN MemoryStore: Not enough space to cache rdd_69_2 in memory! (computed 38.8 MiB so far)\n",
      "22/03/02 15:32:52 WARN MemoryStore: Not enough space to cache rdd_69_0 in memory! (computed 38.8 MiB so far)\n",
      "[Stage 28:==============================================>       (172 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+--------------------+-------------------------------------------+\n",
      "|cnt_rows|PUZone              |DOZone              |pickup_dropoff                             |\n",
      "+--------+--------------------+--------------------+-------------------------------------------+\n",
      "|45041   |East New York       |East New York       |East New York / East New York              |\n",
      "|37329   |Borough Park        |Borough Park        |Borough Park / Borough Park                |\n",
      "|28026   |Canarsie            |Canarsie            |Canarsie / Canarsie                        |\n",
      "|25976   |Crown Heights North |Crown Heights North |Crown Heights North / Crown Heights North  |\n",
      "|17934   |Bay Ridge           |Bay Ridge           |Bay Ridge / Bay Ridge                      |\n",
      "|14688   |Jackson Heights     |Jackson Heights     |Jackson Heights / Jackson Heights          |\n",
      "|14688   |Astoria             |Astoria             |Astoria / Astoria                          |\n",
      "|14481   |Central Harlem North|Central Harlem North|Central Harlem North / Central Harlem North|\n",
      "|14424   |Bushwick South      |Bushwick South      |Bushwick South / Bushwick South            |\n",
      "|13976   |Flatbush/Ditmas Park|Flatbush/Ditmas Park|Flatbush/Ditmas Park / Flatbush/Ditmas Park|\n",
      "+--------+--------------------+--------------------+-------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba0b549",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
